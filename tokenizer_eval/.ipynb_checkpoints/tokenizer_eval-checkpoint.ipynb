{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091f4c03-27f3-433f-8d44-5da41a889c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import json\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aee5c662-4de6-4622-8627-451bc9fb384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = f\"ᑭᓯᐊᓂᖅ{word_separator_token}ᑭ{morph_boundary_token}ᓯᐊᓂᖅ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1af3db6-873d-4e2d-bbd3-4958a7c903a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ᑭᓯᐊᓂᖅ<__word-separator>ᑭ<__morph-boundary>ᓯᐊᓂᖅ'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad43a326-e1aa-44d9-90e9-fb8181291579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁ᑭᓯᐊᓂ',\n",
       " 'ᖅ',\n",
       " '<__word-separator>',\n",
       " '▁ᑭ',\n",
       " '<__morph-boundary>',\n",
       " '▁',\n",
       " 'ᓯᐊ',\n",
       " 'ᓂᖅ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77feb4bc-7561-4a58-b592-0aa7995fe00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "model_name = 'cis-lmu/glot500-base'\n",
    "word_separator_token = \"<__word-separator>\"\n",
    "morph_boundary_token = \"<__morph-boundary>\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d25bc94-38fa-4747-b36a-c77c684941ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.add_tokens([morph_boundary_token, word_separator_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c886e79a-764b-4d6b-a58d-93f025af67bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 401146, 2]\n",
      "[0, 401145, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(word_separator_token))\n",
    "print(tokenizer.encode(morph_boundary_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ead502f-fef0-42f4-924b-de451f974a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data as pandas dataframe\n",
    "path2data_dir = '/home/mathias/Desktop/HI/hpc/inuktitut/llm_segm/reimplementation/data/data_full/test.iu.csv'\n",
    "path2data_out = '/home/mathias/Desktop/HI/hpc/inuktitut/llm_segm/reimplementation/tokenizer_eval'\n",
    "lang = 'iu'\n",
    "data_test = pd.read_csv(path2data_dir, sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0093841c-c3da-46c5-baed-56a4674846ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_word(ground, predicted):\n",
    "    tp = fp = fn = 0\n",
    "    gi = pi = 0\n",
    "    while gi < len(ground) and pi < len(predicted):\n",
    "        g = ground[gi]\n",
    "        p = predicted[pi]\n",
    "        if g == p:\n",
    "            if g == \"@\":\n",
    "                tp += 1\n",
    "            gi += 1\n",
    "            pi += 1\n",
    "        elif g == \"@\":\n",
    "            fn += 1\n",
    "            gi += 1\n",
    "        elif p == \"@\":\n",
    "            fp += 1\n",
    "            pi += 1\n",
    "        else:\n",
    "            assert False, (ground, predicted)\n",
    "    assert gi == len(ground) and pi == len(predicted)\n",
    "    return tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cf5950d-4442-43e5-905f-81ddae927f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenizer(tokenizer, test_data):\n",
    "    TP = FP = FN = 0\n",
    "    ACC = 0\n",
    "    predictions = []\n",
    "    for ground in tqdm.tqdm(test_data):\n",
    "        word = \"\".join(ground.split(\"@\"))\n",
    "        predicted = \"@\".join(tokenizer.tokenize(word))[1:]\n",
    "        #print(ground)\n",
    "        #print(prediction)\n",
    "        #input()\n",
    "        predictions.append(predicted)\n",
    "        if ground == predicted:\n",
    "            ACC += 1\n",
    "        tp, fp, fn = eval_word(ground, predicted)\n",
    "        TP += tp\n",
    "        FP += fp\n",
    "        FN += fn\n",
    "    return ACC, TP, FP, FN, predictions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "084667bf-5e2f-4a9d-8fe0-0f5a53f3d1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 5124/5124 [00:00<00:00, 21733.95it/s]\n"
     ]
    }
   ],
   "source": [
    "ACC, TP, FP, FN, predictions = test_tokenizer(tokenizer, data_test[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db2530e5-2681-4929-9e76-6424ec59fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(ACC, TP, FP, FN, predictions):\n",
    "    predictions = [re.sub(\"@\", \" \", p) for p in predictions]\n",
    "    df = pd.DataFrame({\"word\": data_test[0].tolist(), \"predictions\": predictions})\n",
    "    df.to_csv(f\"{path2data_out}/preds.{lang}.csv\", header=None, index=False, sep=\"\\t\")\n",
    "\n",
    "    P = TP / (TP + FP)\n",
    "    R = TP / (TP + FN)\n",
    "    F1 = 2 * P * R / (P + R)\n",
    "\n",
    "    with open(f\"{path2data_out}/results.{lang}.json\", \"w\") as results_f:\n",
    "            results_map = {\n",
    "                \"ACC\": np.round(100 * ACC / len(data_test), 2),\n",
    "                \"Prec\": np.round(100 * P, 2),\n",
    "                \"Rcl\": np.round(100 * R, 2),\n",
    "                \"F1\": np.round(100 * F1, 2),\n",
    "                \"test-len\": len(data_test),\n",
    "                \"LANG\": lang,\n",
    "            }\n",
    "            json.dump(results_map, results_f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "424731e3-2c6d-4573-b0dd-79ac42418ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁ᐱᓕᕆᖃᑎ', 'ᖃᕐᓂ', 'ᖃ', 'ᖅᑐᑦ']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string1 = \"ᐱᓕᕆᖃᑎᖃᕐᓂᖃᖅᑐᑦ\"\n",
    "tokenizer.tokenize(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ed3312c-4585-4d26-a31c-be2c3a4b5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(ACC, TP, FP, FN, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
